{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "694ff13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0\n",
      "Copyright (C) 2021 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebff35ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.4/620.4 MB\u001b[0m \u001b[31m671.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting opt_einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 KB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (59.6.0)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.76.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorboard~=2.20.0\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.26.0 in ./.local/lib/python3.10/site-packages (from tensorflow) (2.2.6)\n",
      "Collecting h5py>=3.11.0\n",
      "  Downloading h5py-3.15.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers>=24.3.25\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Collecting google_pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (25.0)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.5)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1\n",
      "  Downloading ml_dtypes-0.5.4-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting wrapt>=1.11.0\n",
      "  Downloading wrapt-2.0.1-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (113 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.7/113.7 KB\u001b[0m \u001b[31m434.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf>=5.28.0\n",
      "  Downloading protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.3/323.3 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras>=3.10.0\n",
      "  Downloading keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Collecting optree\n",
      "  Downloading optree-0.18.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (386 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 KB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting namex\n",
      "  Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Collecting rich\n",
      "  Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow) (2020.6.20)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.1.4-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.0/225.0 KB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow in ./.local/lib/python3.10/site-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading markdown-3.10-py3-none-any.whl (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.7/107.7 KB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markupsafe>=2.1.1\n",
      "  Downloading markupsafe-3.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (20 kB)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, termcolor, tensorboard-data-server, protobuf, optree, opt_einsum, ml_dtypes, mdurl, markupsafe, markdown, h5py, grpcio, google_pasta, gast, astunparse, absl-py, werkzeug, markdown-it-py, tensorboard, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.9.23 gast-0.7.0 google_pasta-0.2.0 grpcio-1.76.0 h5py-3.15.1 keras-3.12.0 libclang-18.1.1 markdown-3.10 markdown-it-py-4.0.0 markupsafe-3.0.3 mdurl-0.1.2 ml_dtypes-0.5.4 namex-0.1.0 opt_einsum-3.4.0 optree-0.18.0 protobuf-6.33.2 rich-14.2.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.2.0 werkzeug-3.1.4 wrapt-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b40357ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:51:04.576327: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-09 22:51:04.599970: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-09 22:51:05.493886: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-09 22:51:11.369412: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-09 22:51:11.374772: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
      "Guardando X_train.bin (esto puede tardar unos segundos)...\n",
      "Guardando Y_train.bin...\n",
      "Archivos creados en la carpeta 'data/':\n",
      "X_train.bin: (784, 60000) - float64\n",
      "Y_train.bin: (60000,) - int32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import os\n",
    "\n",
    "# 1. Cargar MNIST\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 2. Preprocesamiento (Igual que en tu notebook original)\n",
    "# Aplanar: (60000, 28, 28) -> (60000, 784)\n",
    "# Normalizar: 0-255 -> 0.0-1.0\n",
    "input_size = 28 * 28\n",
    "X_flat = x_train.reshape(x_train.shape[0], input_size).astype('float64') / 255.0\n",
    "\n",
    "# 3. TRANSPONER para C (Optimización de caché)\n",
    "# Dejamos los datos como (784, 60000) para que C lea columna por columna fácilmente\n",
    "X_train_C = X_flat.T  # Forma: (784, 60000)\n",
    "\n",
    "# 4. Asegurar tipos de datos para C\n",
    "# X debe ser double (8 bytes), Y debe ser int (4 bytes)\n",
    "Y_train_C = y_train.astype('int32')\n",
    "\n",
    "# 5. Guardar en binario puro\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "print(\"Guardando X_train.bin (esto puede tardar unos segundos)...\")\n",
    "X_train_C.tofile('data/X_train.bin')\n",
    "\n",
    "print(\"Guardando Y_train.bin...\")\n",
    "Y_train_C.tofile('data/Y_train.bin')\n",
    "\n",
    "print(f\"Archivos creados en la carpeta 'data/':\")\n",
    "print(f\"X_train.bin: {X_train_C.shape} - {X_train_C.dtype}\")\n",
    "print(f\"Y_train.bin: {Y_train_C.shape} - {Y_train_C.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48fb207d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos MNIST...\n",
      "Datos MNIST cargados exitosamente.\n",
      "----------------------------------------\n",
      "Arquitectura MLP: 784 --> 512 neuronas (Oculta) --> 10 neuronas (Salida).\n",
      "----------------------------------------\n",
      "\n",
      "--- Inicio del Entrenamiento ---\n",
      "Iniciando entrenamiento AVX (Batch Size: 64, LR: 0.01)...\n",
      "Epoch 1/10 - Loss: 0.8182 - Acc: 81.15% - Tiempo: 115.39s\n",
      "Epoch 2/10 - Loss: 0.4003 - Acc: 89.22% - Tiempo: 119.15s\n",
      "Epoch 3/10 - Loss: 0.3390 - Acc: 90.57% - Tiempo: 117.56s\n",
      "Epoch 4/10 - Loss: 0.3065 - Acc: 91.42% - Tiempo: 122.01s\n",
      "Epoch 5/10 - Loss: 0.2836 - Acc: 92.06% - Tiempo: 136.43s\n",
      "Epoch 6/10 - Loss: 0.2654 - Acc: 92.55% - Tiempo: 111.47s\n",
      "Epoch 7/10 - Loss: 0.2500 - Acc: 93.01% - Tiempo: 112.82s\n",
      "Epoch 8/10 - Loss: 0.2366 - Acc: 93.41% - Tiempo: 107.96s\n",
      "Epoch 9/10 - Loss: 0.2248 - Acc: 93.76% - Tiempo: 110.71s\n",
      "Epoch 10/10 - Loss: 0.2142 - Acc: 94.04% - Tiempo: 106.19s\n",
      "\n",
      "----------------------------------------\n",
      "Entrenamiento Finalizado.\n",
      "Tiempo Total AVX: 1159.69 segundos\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!gcc mlp.c -o mlp -lm -mavx2 -mfma\n",
    "!./mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f15cb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mlp.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile mlp.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "#include <time.h>\n",
    "#include <string.h>\n",
    "#include <immintrin.h> // Librería para intrínsecos AVX\n",
    "\n",
    "// --- CONSTANTES DE LA RED ---\n",
    "#define INPUT_SIZE 784\n",
    "#define HIDDEN_SIZE 512   // ¡Tamaño de capa oculta fijado a 512!\n",
    "#define OUTPUT_SIZE 10\n",
    "#define M_TRAIN 60000    \n",
    "#define BATCH_SIZE 64    \n",
    "#define EPOCHS 10        \n",
    "#define LR 0.01\n",
    "\n",
    "// Archivos de datos \n",
    "#define TRAIN_X_PATH \"data/X_train.bin\"\n",
    "#define TRAIN_Y_PATH \"data/Y_train.bin\"\n",
    "\n",
    "// Constante para el ancho de vector AVX2 (4 doubles por registro __m256d)\n",
    "#define AVX_DOUBLE_WIDTH 4\n",
    "\n",
    "// --- ESTRUCTURA Y UTILERÍAS ---\n",
    "\n",
    "typedef struct {\n",
    "    double* W1;\n",
    "    double* b1;\n",
    "    double* W2;\n",
    "    double* b2;\n",
    "} Params;\n",
    "\n",
    "double get_time_diff(clock_t start, clock_t end) {\n",
    "    return (double)(end - start) / CLOCKS_PER_SEC;\n",
    "}\n",
    "\n",
    "// --- CARGA Y PREPARACIÓN DE DATOS ---\n",
    "\n",
    "void load_data(double* X, int* Y) {\n",
    "    FILE *f_x = fopen(TRAIN_X_PATH, \"rb\");\n",
    "    FILE *f_y = fopen(TRAIN_Y_PATH, \"rb\");\n",
    "\n",
    "    if (f_x == NULL || f_y == NULL) {\n",
    "        fprintf(stderr, \"Error: No se encuentran los archivos .bin.\\n\");\n",
    "        exit(1);\n",
    "    }\n",
    "    size_t x_read = fread(X, sizeof(double), INPUT_SIZE * M_TRAIN, f_x);\n",
    "    size_t y_read = fread(Y, sizeof(int), M_TRAIN, f_y);\n",
    "\n",
    "    if (x_read != INPUT_SIZE * M_TRAIN || y_read != M_TRAIN) {\n",
    "        fprintf(stderr, \"Error: Lectura incompleta.\\n\");\n",
    "        exit(1);\n",
    "    }\n",
    "\n",
    "    fclose(f_x);\n",
    "    fclose(f_y);\n",
    "    printf(\"Datos MNIST cargados exitosamente.\\n\");\n",
    "}\n",
    "\n",
    "void init_xavier(double* W, int n_in, int n_out) {\n",
    "    double limit = sqrt(6.0 / (n_in + n_out));\n",
    "    for (int i = 0; i < n_in * n_out; i++) {\n",
    "        W[i] = ((double)rand() / RAND_MAX) * 2 * limit - limit;\n",
    "    }\n",
    "}\n",
    "\n",
    "Params init_params() {\n",
    "    Params p;\n",
    "    srand(42); \n",
    "    p.W1 = (double*)malloc(HIDDEN_SIZE * INPUT_SIZE * sizeof(double));\n",
    "    p.b1 = (double*)calloc(HIDDEN_SIZE, sizeof(double));\n",
    "    p.W2 = (double*)malloc(OUTPUT_SIZE * HIDDEN_SIZE * sizeof(double));\n",
    "    p.b2 = (double*)calloc(OUTPUT_SIZE, sizeof(double));\n",
    "\n",
    "    init_xavier(p.W1, INPUT_SIZE, HIDDEN_SIZE);\n",
    "    init_xavier(p.W2, HIDDEN_SIZE, OUTPUT_SIZE);\n",
    "    return p;\n",
    "}\n",
    "\n",
    "// --- IMPLEMENTACIÓN AVX2/FMA OPTIMIZADA ---\n",
    "\n",
    "/**\n",
    " * Suma horizontal de los 4 doubles de un registro __m256d.\n",
    " */\n",
    "double hsum_avx(__m256d v) {\n",
    "    __m128d v128 = _mm_add_pd(_mm256_castpd256_pd128(v), _mm256_extractf128_pd(v, 1));\n",
    "    __m128d v64 = _mm_hadd_pd(v128, v128);\n",
    "    return _mm_cvtsd_f64(v64);\n",
    "}\n",
    "\n",
    "/**\n",
    " * C = A * B. AVX optimizado para el Forward Pass (W * X).\n",
    " * La optimización es limitada porque el acceso a la matriz B (X) no es contiguo\n",
    " * por columnas, lo que requiere cargas manuales menos eficientes.\n",
    " */\n",
    "void matmul_avx(const double* A, const double* B, double* C, int m, int n, int p) {\n",
    "    for (int i = 0; i < m; i++) { \n",
    "        for (int j = 0; j < p; j++) { \n",
    "            __m256d sum_vec = _mm256_setzero_pd();\n",
    "            double sum_scalar = 0.0;\n",
    "            int k;\n",
    "\n",
    "            // Bucle vectorial (k) - Reduce en la dimensión I o H\n",
    "            for (k = 0; k < (n / AVX_DOUBLE_WIDTH) * AVX_DOUBLE_WIDTH; k += AVX_DOUBLE_WIDTH) {\n",
    "                __m256d a_vec = _mm256_loadu_pd(&A[i * n + k]);\n",
    "                \n",
    "                // Carga no contigua de la columna de B (X)\n",
    "                double B_temp[AVX_DOUBLE_WIDTH];\n",
    "                B_temp[0] = B[(k + 0) * p + j];\n",
    "                B_temp[1] = B[(k + 1) * p + j];\n",
    "                B_temp[2] = B[(k + 2) * p + j];\n",
    "                B_temp[3] = B[(k + 3) * p + j];\n",
    "                __m256d b_vec = _mm256_loadu_pd(B_temp); \n",
    "\n",
    "                // ¡Uso de FMA! (FUSED MULTIPLY-ADD)\n",
    "                sum_vec = _mm256_fmadd_pd(a_vec, b_vec, sum_vec); \n",
    "            }\n",
    "            \n",
    "            sum_scalar = hsum_avx(sum_vec);\n",
    "\n",
    "            // Sobrantes (escalar)\n",
    "            for (; k < n; k++) {\n",
    "                sum_scalar += A[i * n + k] * B[k * p + j];\n",
    "            }\n",
    "\n",
    "            C[i * p + j] = sum_scalar;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    " * C = A * B^T. Utilizado para dW = dZ * A^T. (¡MÁXIMA OPTIMIZACIÓN SECUENCIAL!)\n",
    " * Las filas de A (dZ) y B (A o X) son contiguas, lo que maximiza la eficiencia AVX.\n",
    " */\n",
    "void matmul_Bt_avx(const double* A, const double* B, double* C, int m, int n, int p) {\n",
    "    for (int i = 0; i < m; i++) { \n",
    "        for (int j = 0; j < p; j++) { \n",
    "            __m256d sum_vec = _mm256_setzero_pd();\n",
    "            double sum_scalar = 0.0;\n",
    "            int k;\n",
    "            \n",
    "            // Bucle vectorial (k)\n",
    "            for (k = 0; k < (n / AVX_DOUBLE_WIDTH) * AVX_DOUBLE_WIDTH; k += AVX_DOUBLE_WIDTH) {\n",
    "                __m256d a_vec = _mm256_loadu_pd(&A[i * n + k]); // Carga contigua de A\n",
    "                __m256d b_vec = _mm256_loadu_pd(&B[j * n + k]); // Carga contigua de B (como fila)\n",
    "                \n",
    "                // ¡FMA!\n",
    "                sum_vec = _mm256_fmadd_pd(a_vec, b_vec, sum_vec); \n",
    "            }\n",
    "            \n",
    "            sum_scalar = hsum_avx(sum_vec);\n",
    "\n",
    "            // Sobrantes (escalar)\n",
    "            for (; k < n; k++) {\n",
    "                sum_scalar += A[i * n + k] * B[j * n + k];\n",
    "            }\n",
    "            C[i * p + j] = sum_scalar;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    " * C = A^T * B. Utilizado para dA1 = W2^T * dZ2.\n",
    " * Vectoriza sobre las columnas de B (dimensión de batch).\n",
    " */\n",
    "void matmul_At_avx(const double* A, const double* B, double* C, int m, int n, int p) {\n",
    "    for (int i = 0; i < m; i++) { \n",
    "        int j;\n",
    "        // Bucle vectorial (j) - Columnas (dimensión de Batch)\n",
    "        for (j = 0; j < (p / AVX_DOUBLE_WIDTH) * AVX_DOUBLE_WIDTH; j += AVX_DOUBLE_WIDTH) { \n",
    "            __m256d c_vec = _mm256_setzero_pd();\n",
    "            for (int k = 0; k < n; k++) { \n",
    "                // A[k, i] es un escalar, se replica 4 veces\n",
    "                __m256d a_scalar_vec = _mm256_set1_pd(A[k * m + i]); \n",
    "                __m256d b_vec = _mm256_loadu_pd(&B[k * p + j]);\n",
    "                \n",
    "                // ¡FMA!\n",
    "                c_vec = _mm256_fmadd_pd(a_scalar_vec, b_vec, c_vec);\n",
    "            }\n",
    "            _mm256_storeu_pd(&C[i * p + j], c_vec);\n",
    "        }\n",
    "        // Sobrantes (escalar)\n",
    "        for (int j_scalar = j; j_scalar < p; j_scalar++) {\n",
    "            double sum = 0.0;\n",
    "            for (int k = 0; k < n; k++) {\n",
    "                sum += A[k * m + i] * B[k * p + j_scalar];\n",
    "            }\n",
    "            C[i * p + j_scalar] = sum;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "void add_bias_avx(double* Z, const double* b, int rows, int cols) {\n",
    "    for (int i = 0; i < rows; i++) {\n",
    "        __m256d b_scalar_vec = _mm256_set1_pd(b[i]);\n",
    "        int j;\n",
    "        for (j = 0; j < (cols / AVX_DOUBLE_WIDTH) * AVX_DOUBLE_WIDTH; j += AVX_DOUBLE_WIDTH) {\n",
    "            __m256d z_vec = _mm256_loadu_pd(&Z[i * cols + j]);\n",
    "            z_vec = _mm256_add_pd(z_vec, b_scalar_vec);\n",
    "            _mm256_storeu_pd(&Z[i * cols + j], z_vec);\n",
    "        }\n",
    "        for (; j < cols; j++) {\n",
    "            Z[i * cols + j] += b[i];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "void relu_backward_avx(double* dZ, const double* Z, int size) {\n",
    "    __m256d zero_vec = _mm256_setzero_pd();\n",
    "    int i;\n",
    "    for (i = 0; i < (size / AVX_DOUBLE_WIDTH) * AVX_DOUBLE_WIDTH; i += AVX_DOUBLE_WIDTH) {\n",
    "        __m256d dz_vec = _mm256_loadu_pd(&dZ[i]);\n",
    "        __m256d z_vec = _mm256_loadu_pd(&Z[i]);\n",
    "        \n",
    "        __m256d mask = _mm256_cmp_pd(z_vec, zero_vec, _CMP_GT_OS); \n",
    "        __m256d result_vec = _mm256_and_pd(dz_vec, mask);\n",
    "        \n",
    "        _mm256_storeu_pd(&dZ[i], result_vec);\n",
    "    }\n",
    "    for (; i < size; i++) {\n",
    "        if (Z[i] <= 0) dZ[i] = 0;\n",
    "    }\n",
    "}\n",
    "\n",
    "void update_params_avx(double* W, const double* dW, const double inv_m, int size) {\n",
    "    __m256d lr_vec = _mm256_set1_pd(LR * inv_m);\n",
    "    int i;\n",
    "    for (i = 0; i < (size / AVX_DOUBLE_WIDTH) * AVX_DOUBLE_WIDTH; i += AVX_DOUBLE_WIDTH) {\n",
    "        __m256d w_vec = _mm256_loadu_pd(&W[i]);\n",
    "        __m256d dw_vec = _mm256_loadu_pd(&dW[i]);\n",
    "        \n",
    "        // W -= LR_inv_m * dW. Se usa FMA aquí también para el vector de paso de gradiente.\n",
    "        __m256d step_vec = _mm256_mul_pd(lr_vec, dw_vec);\n",
    "        w_vec = _mm256_sub_pd(w_vec, step_vec);\n",
    "        \n",
    "        _mm256_storeu_pd(&W[i], w_vec);\n",
    "    }\n",
    "    for (; i < size; i++) {\n",
    "        W[i] -= LR * dW[i] * inv_m;\n",
    "    }\n",
    "}\n",
    "\n",
    "// --- FUNCIONES ESCALARES ---\n",
    "\n",
    "void relu(double* Z, int size) {\n",
    "    for (int i = 0; i < size; i++) {\n",
    "        if (Z[i] < 0) Z[i] = 0;\n",
    "    }\n",
    "}\n",
    "\n",
    "void softmax(double* Z, int rows, int cols) {\n",
    "    for (int j = 0; j < cols; j++) {\n",
    "        double max = Z[0 * cols + j];\n",
    "        for (int i = 1; i < rows; i++) {\n",
    "            if (Z[i * cols + j] > max) max = Z[i * cols + j];\n",
    "        }\n",
    "        double sum = 0.0;\n",
    "        for (int i = 0; i < rows; i++) {\n",
    "            Z[i * cols + j] = exp(Z[i * cols + j] - max); \n",
    "            sum += Z[i * cols + j];\n",
    "        }\n",
    "        for (int i = 0; i < rows; i++) {\n",
    "            Z[i * cols + j] /= sum;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "void one_hot(const int* Y, double* Y_OH, int m_batch) {\n",
    "    memset(Y_OH, 0, OUTPUT_SIZE * m_batch * sizeof(double));\n",
    "    for (int j = 0; j < m_batch; j++) {\n",
    "        int label = Y[j];\n",
    "        if (label >= 0 && label < OUTPUT_SIZE) {\n",
    "            Y_OH[label * m_batch + j] = 1.0;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "double get_accuracy(const double* A2, const int* Y, int m_batch) {\n",
    "    int correct_predictions = 0;\n",
    "    for (int j = 0; j < m_batch; j++) {\n",
    "        double max_val = -1.0;\n",
    "        int predicted_label = -1;\n",
    "        for (int i = 0; i < OUTPUT_SIZE; i++) {\n",
    "            if (A2[i * m_batch + j] > max_val) {\n",
    "                max_val = A2[i * m_batch + j];\n",
    "                predicted_label = i;\n",
    "            }\n",
    "        }\n",
    "        if (predicted_label == Y[j]) {\n",
    "            correct_predictions++;\n",
    "        }\n",
    "    }\n",
    "    return (double)correct_predictions / m_batch;\n",
    "}\n",
    "\n",
    "double cross_entropy_loss(const double* A2, const int* Y, int m_batch) {\n",
    "    double loss = 0.0;\n",
    "    for (int j = 0; j < m_batch; j++) {\n",
    "        int true_label = Y[j];\n",
    "        \n",
    "        double prob = A2[true_label * m_batch + j];\n",
    "        \n",
    "        if (prob < 1e-12) {\n",
    "            prob = 1e-12; \n",
    "        }\n",
    "\n",
    "        loss += -log(prob);\n",
    "    }\n",
    "    return loss / m_batch;\n",
    "}\n",
    "\n",
    "// --- BUCLE DE ENTRENAMIENTO PRINCIPAL ---\n",
    "\n",
    "void train(Params p, double* X_train, int* Y_train) {\n",
    "    // Asignación de Buffers de Memoria\n",
    "    double* Z1 = malloc(HIDDEN_SIZE * BATCH_SIZE * sizeof(double));\n",
    "    double* A1 = malloc(HIDDEN_SIZE * BATCH_SIZE * sizeof(double));\n",
    "    double* Z2 = malloc(OUTPUT_SIZE * BATCH_SIZE * sizeof(double));\n",
    "    double* dZ2 = malloc(OUTPUT_SIZE * BATCH_SIZE * sizeof(double));\n",
    "    double* dW2 = malloc(OUTPUT_SIZE * HIDDEN_SIZE * sizeof(double));\n",
    "    double* db2 = malloc(OUTPUT_SIZE * sizeof(double));\n",
    "    double* dA1 = malloc(HIDDEN_SIZE * BATCH_SIZE * sizeof(double));\n",
    "    double* dW1 = malloc(HIDDEN_SIZE * INPUT_SIZE * sizeof(double));\n",
    "    double* db1 = malloc(HIDDEN_SIZE * sizeof(double));\n",
    "    double* Y_batch_oh = malloc(OUTPUT_SIZE * BATCH_SIZE * sizeof(double));\n",
    "\n",
    "    int num_batches = M_TRAIN / BATCH_SIZE;\n",
    "\n",
    "    printf(\"\\n--- Inicio del Entrenamiento ---\\n\");\n",
    "    printf(\"Iniciando entrenamiento AVX (Batch Size: %d, LR: %.2f)...\\n\", BATCH_SIZE, LR);\n",
    "\n",
    "\n",
    "    for (int epoch = 0; epoch < EPOCHS; epoch++) {\n",
    "        int correct = 0;\n",
    "        double epoch_loss = 0.0;\n",
    "        clock_t ep_start = clock();\n",
    "\n",
    "        for (int b = 0; b < num_batches; b++) {\n",
    "            // 1. Extracción del Batch (Copia y transposición para un layout óptimo)\n",
    "            double* X_batch_ptr = malloc(INPUT_SIZE * BATCH_SIZE * sizeof(double));\n",
    "            for(int i=0; i<INPUT_SIZE; i++) {\n",
    "                memcpy(&X_batch_ptr[i * BATCH_SIZE], \n",
    "                       &X_train[i * M_TRAIN + b * BATCH_SIZE], \n",
    "                       BATCH_SIZE * sizeof(double));\n",
    "            }\n",
    "            int* Y_batch_ptr = &Y_train[b * BATCH_SIZE];\n",
    "\n",
    "            // 2. FORWARD PROPAGATION\n",
    "            // Z1 = W1 * X + b1\n",
    "            matmul_avx(p.W1, X_batch_ptr, Z1, HIDDEN_SIZE, INPUT_SIZE, BATCH_SIZE);\n",
    "            add_bias_avx(Z1, p.b1, HIDDEN_SIZE, BATCH_SIZE); \n",
    "            \n",
    "            memcpy(A1, Z1, HIDDEN_SIZE * BATCH_SIZE * sizeof(double));\n",
    "            relu(A1, HIDDEN_SIZE * BATCH_SIZE); \n",
    "\n",
    "            // Z2 = W2 * A1 + b2\n",
    "            matmul_avx(p.W2, A1, Z2, OUTPUT_SIZE, HIDDEN_SIZE, BATCH_SIZE);\n",
    "            add_bias_avx(Z2, p.b2, OUTPUT_SIZE, BATCH_SIZE); \n",
    "\n",
    "            // A2 = Softmax(Z2)\n",
    "            softmax(Z2, OUTPUT_SIZE, BATCH_SIZE); \n",
    "\n",
    "            // 3. CÁLCULO DE PÉRDIDA Y ACCURACY\n",
    "            epoch_loss += cross_entropy_loss(Z2, Y_batch_ptr, BATCH_SIZE);\n",
    "            correct += get_accuracy(Z2, Y_batch_ptr, BATCH_SIZE) * BATCH_SIZE;\n",
    "\n",
    "            // 4. BACKWARD PROPAGATION\n",
    "            one_hot(Y_batch_ptr, Y_batch_oh, BATCH_SIZE);\n",
    "            \n",
    "            // dZ2 = A2 - Y_OH\n",
    "            for(int i=0; i<OUTPUT_SIZE * BATCH_SIZE; i++) dZ2[i] = Z2[i] - Y_batch_oh[i];\n",
    "\n",
    "            // dW2 = (1/m) * dZ2 * A1^T (Máxima Optimización AVX/FMA)\n",
    "            matmul_Bt_avx(dZ2, A1, dW2, OUTPUT_SIZE, BATCH_SIZE, HIDDEN_SIZE);\n",
    "            \n",
    "            // db2 = (1/m) * sum(dZ2) (Escalar)\n",
    "            for(int i=0; i<OUTPUT_SIZE; i++) {\n",
    "                db2[i] = 0;\n",
    "                for(int j=0; j<BATCH_SIZE; j++) db2[i] += dZ2[i * BATCH_SIZE + j];\n",
    "            }\n",
    "\n",
    "            // dA1 = W2^T * dZ2 (Optimizado AVX/FMA)\n",
    "            matmul_At_avx(p.W2, dZ2, dA1, HIDDEN_SIZE, OUTPUT_SIZE, BATCH_SIZE);\n",
    "\n",
    "            // dZ1 = dA1 * ReLU'(Z1) (Optimizado AVX)\n",
    "            relu_backward_avx(dA1, Z1, HIDDEN_SIZE * BATCH_SIZE); \n",
    "\n",
    "            // dW1 = (1/m) * dZ1 * X^T (Máxima Optimización AVX/FMA)\n",
    "            matmul_Bt_avx(dA1, X_batch_ptr, dW1, HIDDEN_SIZE, BATCH_SIZE, INPUT_SIZE);\n",
    "            \n",
    "            // db1 = (1/m) * sum(dZ1) (Escalar)\n",
    "            for(int i=0; i<HIDDEN_SIZE; i++) {\n",
    "                db1[i] = 0;\n",
    "                for(int j=0; j<BATCH_SIZE; j++) db1[i] += dA1[i * BATCH_SIZE + j];\n",
    "            }\n",
    "\n",
    "            // 5. UPDATE PARAMETERS (Optimizado AVX/FMA)\n",
    "            double inv_m = 1.0 / BATCH_SIZE;\n",
    "            \n",
    "            update_params_avx(p.W2, dW2, inv_m, OUTPUT_SIZE * HIDDEN_SIZE);\n",
    "            for(int i=0; i<OUTPUT_SIZE; i++) p.b2[i] -= LR * db2[i] * inv_m; \n",
    "            \n",
    "            update_params_avx(p.W1, dW1, inv_m, HIDDEN_SIZE * INPUT_SIZE);\n",
    "            for(int i=0; i<HIDDEN_SIZE; i++) p.b1[i] -= LR * db1[i] * inv_m; \n",
    "\n",
    "            free(X_batch_ptr);\n",
    "        }\n",
    "        \n",
    "        clock_t ep_end = clock();\n",
    "        double avg_epoch_loss = epoch_loss / num_batches; \n",
    "        \n",
    "        printf(\"Epoch %d/%d - Loss: %.4f - Acc: %.2f%% - Tiempo: %.2fs\\n\", \n",
    "               epoch+1, EPOCHS, avg_epoch_loss, (double)correct * 100.0 / M_TRAIN, get_time_diff(ep_start, ep_end));\n",
    "    }\n",
    "\n",
    "    // Liberación de Buffers\n",
    "    free(Z1); free(A1); free(Z2); free(dZ2); free(dW2); free(db2);\n",
    "    free(dA1); free(dW1); free(db1); free(Y_batch_oh);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    double* X_train = malloc(INPUT_SIZE * M_TRAIN * sizeof(double));\n",
    "    int* Y_train = malloc(M_TRAIN * sizeof(int));\n",
    "\n",
    "    if (!X_train || !Y_train) {\n",
    "        fprintf(stderr, \"Error de memoria.\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "\n",
    "    printf(\"Cargando datos MNIST...\\n\");\n",
    "    load_data(X_train, Y_train);\n",
    "\n",
    "    // --- MUESTRA DE ARQUITECTURA ---\n",
    "    printf(\"----------------------------------------\\n\");\n",
    "    printf(\"Arquitectura MLP: %d --> %d neuronas (Oculta) --> %d neuronas (Salida).\\n\", \n",
    "           INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE);\n",
    "    printf(\"----------------------------------------\\n\");\n",
    "    \n",
    "    Params p = init_params();\n",
    "    \n",
    "    clock_t start = clock();\n",
    "    \n",
    "    train(p, X_train, Y_train);\n",
    "    \n",
    "    clock_t end = clock();\n",
    "    double total_time = get_time_diff(start, end);\n",
    "\n",
    "    printf(\"\\n----------------------------------------\\n\");\n",
    "    printf(\"Entrenamiento Finalizado.\\n\");\n",
    "    printf(\"Tiempo Total AVX: %.2f segundos\\n\", total_time);\n",
    "    printf(\"----------------------------------------\\n\");\n",
    "\n",
    "    // Liberación de Parámetros y Datos\n",
    "    free(X_train); free(Y_train);\n",
    "    free(p.W1); free(p.b1); free(p.W2); free(p.b2);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
